{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 20:44:30.513360: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-15 20:44:30.919800: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-15 20:44:32.798103: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/maverick/Downloads/courses/cs685/ProjectWorkspace/models/modelenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: SchuylerH/bert-multilingual-go-emtions...\n",
      "Model SchuylerH/bert-multilingual-go-emtions loaded on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maverick/Downloads/courses/cs685/ProjectWorkspace/models/modelenv/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "testedModels = [\n",
    "    # \"bhadresh-savani/bert-base-go-emotion\",\n",
    "    \"SchuylerH/bert-multilingual-go-emtions\",\n",
    "    # \"joeddav/distilbert-base-uncased-go-emotions-student\",\n",
    "    # \"SamLowe/roberta-base-go_emotions\",\n",
    "    # # below two are good models, can refer documentation for more details\n",
    "    # \"michellejieli/emotion_text_classifier\",\n",
    "    # \"j-hartmann/emotion-english-distilroberta-base\",\n",
    "    # \"gokuls/BERT-tiny-emotion-intent\"\n",
    "]\n",
    "\n",
    "models = {}\n",
    "def LoadModels():\n",
    "    # 1. Load Pre-trained Model for Emotion Classification\n",
    "    # loading all models in for loop\n",
    "    for model_name in testedModels:\n",
    "        print(f\"Loading model: {model_name}...\")\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        \n",
    "        # Move model to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"Model {model_name} loaded on GPU.\")\n",
    "            model.to(\"cuda\")\n",
    "        else:\n",
    "            print(f\"GPU not available. Model {model_name} will run on CPU.\")\n",
    "        \n",
    "\n",
    "        \n",
    "        # High-level pipeline with GPU enabled\n",
    "        classifier = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=0 if torch.cuda.is_available() else -1,  # Device 0 for GPU, -1 for CPU\n",
    "            return_all_scores=True,\n",
    "        )\n",
    "        \n",
    "        models[model_name] = {\"classifier\": classifier}\n",
    "LoadModels()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/maverick/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/maverick/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import clean\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# file = 'data/twitter-stream-2021-06-23_filtered.json'\n",
    "\n",
    "# data1 = []\n",
    "# data2 = []\n",
    "\n",
    "# with open(file) as f:\n",
    "#     for line in f:\n",
    "#         try:\n",
    "#             texts = json.loads(line)['output']['Objects']\n",
    "#             for i in range(0, len(texts)):\n",
    "#                 data1.append(texts[i]['text'])\n",
    "#                 data2.append(clean.preprocess_tweet(texts[i]['text']))\n",
    "#         except json.JSONDecodeError as e:\n",
    "#             print(f\"Error decoding JSON in {file}: {e}\")\n",
    "\n",
    "# # print(data)\n",
    "\n",
    "# # redirect data to json format\n",
    "# with open('data/dump_withStopWords.json', 'w') as outfile:\n",
    "#     for entry in data2:\n",
    "#         json.dump(entry, outfile)\n",
    "#         outfile.write('\\n')\n",
    "\n",
    "# with open('data/dump.json', 'w') as outfile:\n",
    "#     for entry in data1:\n",
    "#         json.dump(entry, outfile)\n",
    "#         outfile.write('\\n')\n",
    "# print('File written')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take two lines at a time from both files\n",
    "\n",
    "# create folder data_cleaned if it doesn't exist\n",
    "import os\n",
    "# if not os.path.exists('data_cleaned'):\n",
    "#     os.makedirs('data_cleaned')\n",
    "\n",
    "# input_dir = 'New'\n",
    "# output_dir = 'data_cleaned'\n",
    "\n",
    "\n",
    "# for file in os.listdir(input_dir):\n",
    "#     with open(os.path.join(input_dir, file)) as f1:\n",
    "#         with open(os.path.join(output_dir, file), 'w') as f2:\n",
    "#             for line1 in f1:\n",
    "#                 # get the text from the line\n",
    "#                 text1 = json.loads(line1)\n",
    "#                 new_twts = []\n",
    "\n",
    "#                 for twt in text1['output']['Objects']:\n",
    "#                     new_twt = twt.copy()\n",
    "#                     new_twt['text'] = clean.preprocess_tweet(twt['text'])\n",
    "#                     new_twts.append(new_twt)\n",
    "                \n",
    "#                 text1['output']['Objects'] = new_twts\n",
    "\n",
    "#                 # dump this new text to a new file\n",
    "#                 json.dump(text1, f2)\n",
    "#                 f2.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the categories of emotions\n",
    "# Emotion to class name mapping\n",
    "# Emotion to category mapping\n",
    "emotion_to_category = {\n",
    "    \"admiration\": \"Positive or Joyful Emotions\",\n",
    "    \"amusement\": \"Positive or Joyful Emotions\",\n",
    "    \"joy\": \"Positive or Joyful Emotions\",\n",
    "    \"gratitude\": \"Positive or Joyful Emotions\",\n",
    "    \"excitement\": \"Positive or Joyful Emotions\",\n",
    "    \"optimism\": \"Positive or Joyful Emotions\",\n",
    "    \"pride\": \"Positive or Joyful Emotions\",\n",
    "    \"relief\": \"Positive or Joyful Emotions\",\n",
    "    \"anger\": \"Negative or Fearful Emotions\",\n",
    "    \"annoyance\": \"Negative or Fearful Emotions\",\n",
    "    \"fear\": \"Negative or Fearful Emotions\",\n",
    "    \"nervousness\": \"Negative or Fearful Emotions\",\n",
    "    \"sadness\": \"Negative or Fearful Emotions\",\n",
    "    \"grief\": \"Negative or Fearful Emotions\",\n",
    "    \"remorse\": \"Negative or Fearful Emotions\",\n",
    "    \"neutral\": \"Neutral or Objective Emotions\",\n",
    "    \"approval\": \"Neutral or Objective Emotions\",\n",
    "    \"realization\": \"Neutral or Objective Emotions\",\n",
    "    \"love\": \"Love and Caring Emotions\",\n",
    "    \"caring\": \"Love and Caring Emotions\",\n",
    "    \"desire\": \"Love and Caring Emotions\",\n",
    "    \"surprise\": \"Surprise or Unexpected Responses\",\n",
    "    \"curiosity\": \"Surprise or Unexpected Responses\",\n",
    "    \"confusion\": \"Surprise or Unexpected Responses\",\n",
    "    \"disappointment\": \"Displeasure or Disapproval Emotions\",\n",
    "    \"disapproval\": \"Displeasure or Disapproval Emotions\",\n",
    "    \"disgust\": \"Displeasure or Disapproval Emotions\",\n",
    "    \"embarrassment\": \"Displeasure or Disapproval Emotions\"\n",
    "}\n",
    "\n",
    "# Category to emotions mapping\n",
    "category_to_emotions = {\n",
    "    \"Positive or Joyful Emotions\": [\n",
    "        \"admiration\", \"amusement\", \"joy\", \"gratitude\", \"excitement\", \n",
    "        \"optimism\", \"pride\", \"relief\"\n",
    "    ],\n",
    "    \"Negative or Fearful Emotions\": [\n",
    "        \"anger\", \"annoyance\", \"fear\", \"nervousness\", \"sadness\", \n",
    "        \"grief\", \"remorse\"\n",
    "    ],\n",
    "    \"Neutral or Objective Emotions\": [\n",
    "        \"neutral\", \"approval\", \"realization\"\n",
    "    ],  \n",
    "    \"Love and Caring Emotions\": [\n",
    "        \"love\", \"caring\", \"desire\"\n",
    "    ],\n",
    "    \"Surprise or Unexpected Responses\": [\n",
    "        \"surprise\", \"curiosity\", \"confusion\"\n",
    "    ],\n",
    "    \"Displeasure or Disapproval Emotions\": [\n",
    "        \"disappointment\", \"disapproval\", \"disgust\",\"embarrassment\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "# change to parent emotion function from single result obj\n",
    "def change_to_parent_emotion(res):\n",
    "    res['label'] = emotion_to_category[res['label']]\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File twitter_stream_2020_01_07_filtered.json already processed\n",
      "File twitter-stream-2021-05-24_filtered.json already processed\n",
      "File twitter-stream-20211004_filtered.json already processed\n",
      "File twitter-stream-2020-09-22_filtered.json already processed\n",
      "File twitter-stream-2020-11-21_filtered.json already processed\n",
      "File twitter-stream-2020-07-12_filtered.json already processed\n",
      "File twitter-stream-2020-12-21_filtered.json already processed\n",
      "File twitter-stream-20211028_filtered.json already processed\n",
      "File twitter-stream-2021-02-25_filtered.json already processed\n",
      "File twitter-stream-2020-10-16_filtered.json already processed\n",
      "File twitter-stream-2021-03-03_filtered.json already processed\n",
      "File twitter-stream-2021-07-05_filtered.json already processed\n",
      "File twitter-stream-2020-12-03_filtered.json already processed\n",
      "File twitter-stream-20211022_filtered.json already processed\n",
      "File twitter-stream-20211127_filtered.json already processed\n",
      "File twitter-stream-2021-01-26_filtered.json already processed\n",
      "File twitter_stream_2020_06_12_filtered.json already processed\n",
      "File twitter-stream-20211016_filtered.json already processed\n",
      "File twitter_stream_2020_06_24_filtered.json already processed\n",
      "File twitter-stream-20211103_filtered.json already processed\n",
      "File twitter-stream-2020-11-09_filtered.json already processed\n",
      "File twitter-stream-2021-08-17_filtered.json already processed\n",
      "File twitter-stream-2020-08-17_filtered.json already processed\n",
      "File twitter-stream-2020-12-15_filtered.json already processed\n",
      "File twitter_stream_2020_05_01_filtered.json already processed\n",
      "File twitter-stream-2021-06-17_filtered.json already processed\n",
      "File twitter-stream-2021-02-19_filtered.json already processed\n",
      "File twitter-stream-2021-08-05_filtered.json already processed\n",
      "File twitter-stream-2020-10-10_filtered.json already processed\n",
      "File twitter-stream-2020-12-09_filtered.json already processed\n",
      "File twitter-stream-2021-03-09_filtered.json already processed\n",
      "File twitter-stream-20210910_filtered.json already processed\n",
      "File twitter-stream-2020-11-03_filtered.json already processed\n",
      "File twitter_stream_2020_05_19_filtered.json already processed\n",
      "File twitter-stream-20210904_filtered.json already processed\n",
      "File twitter-stream-20210922_filtered.json already processed\n",
      "File twitter-stream-20211227_filtered.json already processed\n",
      "File twitter-stream-2020-08-05_filtered.json already processed\n",
      "File twitter-stream-2021-05-18_filtered.json already processed\n",
      "File twitter-stream-2021-02-01_filtered.json already processed\n",
      "File twitter-stream-20210823_filtered.json already processed\n",
      "File twitter_stream_2020_04_14_filtered.json already processed\n",
      "File twitter_stream_2020_06_06_filtered.json written\n",
      "File twitter-stream-2020-12-27_filtered.json written\n",
      "File twitter-stream-2020-07-24_filtered.json written\n",
      "File twitter-stream-20211215_filtered.json written\n",
      "File twitter-stream-2020-07-30_filtered.json written\n",
      "File twitter-stream-2021-02-07_filtered.json written\n",
      "File twitter_stream_2020_03_03_filtered.json written\n",
      "File twitter_stream_2020_04_26_filtered.json written\n",
      "File twitter-stream-20210829_filtered.json written\n",
      "File twitter-stream-2020-09-04_filtered.json written\n",
      "File twitter-stream-2020-08-29_filtered.json written\n",
      "File twitter-stream-20211209_filtered.json written\n",
      "File twitter-stream-2020-10-22_filtered.json written\n",
      "File twitter-stream-20211010_filtered.json written\n",
      "File twitter_stream_2020_03_21_filtered.json written\n",
      "File twitter-stream-20210916_filtered.json written\n",
      "File twitter-stream-2021-06-05_filtered.json written\n",
      "File twitter-stream-2021-05-30_filtered.json written\n",
      "File twitter-stream-2020-09-28_filtered.json written\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m         tcount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Batch process the texts to classify emotions\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     batch_results \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust batch size based on GPU memory\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# print(len(text_count))\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# print(tcount)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(inputdir, file)) \u001b[38;5;28;01mas\u001b[39;00m f1, \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(outputdir, file), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f2:\n\u001b[1;32m     46\u001b[0m     \n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Update the original data with emotion results\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/courses/cs685/ProjectWorkspace/models/modelenv/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:159\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,)\n\u001b[0;32m--> 159\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/Downloads/courses/cs685/ProjectWorkspace/models/modelenv/lib/python3.8/site-packages/transformers/pipelines/base.py:1283\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1280\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1281\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1282\u001b[0m     )\n\u001b[0;32m-> 1283\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Downloads/courses/cs685/ProjectWorkspace/models/modelenv/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/courses/cs685/ProjectWorkspace/models/modelenv/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/courses/cs685/ProjectWorkspace/models/modelenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Downloads/courses/cs685/ProjectWorkspace/models/modelenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Downloads/courses/cs685/ProjectWorkspace/models/modelenv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Downloads/courses/cs685/ProjectWorkspace/models/modelenv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Downloads/courses/cs685/ProjectWorkspace/models/modelenv/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:19\u001b[0m, in \u001b[0;36mPipelineDataset.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[1;32m     18\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[i]\n\u001b[0;32m---> 19\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m processed\n",
      "File \u001b[0;32m~/Downloads/courses/cs685/ProjectWorkspace/models/modelenv/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:183\u001b[0m, in \u001b[0;36mTextClassificationPipeline.preprocess\u001b[0;34m(self, inputs, **tokenizer_kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# This is likely an invalid usage of the pipeline attempting to pass text pairs.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe pipeline received invalid inputs, if you are trying to send text pairs, you can try to send a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m dictionary `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy text\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_pair\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy pair\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}` in order to send a text pair.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/courses/cs685/ProjectWorkspace/models/modelenv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3021\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   3020\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 3021\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3023\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/Downloads/courses/cs685/ProjectWorkspace/models/modelenv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3131\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   3110\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   3111\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3128\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3129\u001b[0m     )\n\u001b[1;32m   3130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3134\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3150\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3151\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/courses/cs685/ProjectWorkspace/models/modelenv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3207\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3197\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3198\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3199\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3200\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3204\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3205\u001b[0m )\n\u001b[0;32m-> 3207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3210\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3226\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3227\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3228\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/courses/cs685/ProjectWorkspace/models/modelenv/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:603\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    581\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    601\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    602\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 603\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/Downloads/courses/cs685/ProjectWorkspace/models/modelenv/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:529\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m!=\u001b[39m split_special_tokens:\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m=\u001b[39m split_special_tokens\n\u001b[0;32m--> 529\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    541\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    543\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    553\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # two files, dump_withStopWords.json and dump_noStopWords.json\n",
    "# # both contain the real tweet, and the tweet after preprocessing, make table of results for each models\n",
    "\n",
    "\n",
    "# # 3. Classify Emotions\n",
    "# results = {}\n",
    "inputdir = 'data_cleaned'\n",
    "# file = 'twitter-stream-2021-06-23_filtered.json'\n",
    "outputdir = 'data_with_emotions'\n",
    "\n",
    "# # for model_name in models:\n",
    "# model_name = testedModels[0]\n",
    "\n",
    "# # create folder data_with_emotions if it doesn't exist\n",
    "# if not os.path.exists(outputdir):\n",
    "#     os.makedirs(outputdir)\n",
    "\n",
    "# # take two lines at a time from both files\n",
    "classifier = models[model_name][\"classifier\"]\n",
    "# # Process files\n",
    "# batch_results = []\n",
    "# for file in os.listdir(inputdir):\n",
    "#     if file in os.listdir(outputdir):\n",
    "#         print(f\"File {file} already processed\")\n",
    "#         continue\n",
    "#     with open(os.path.join(inputdir, file)) as f1:\n",
    "#         # Collect all texts from the input file for batch processing\n",
    "#         texts = []\n",
    "#         text_count = []\n",
    "#         tcount = 0\n",
    "#         for line in f1:\n",
    "#             text_entry = json.loads(line)\n",
    "#             count = 0\n",
    "#             for obj in text_entry['output']['Objects']:\n",
    "#                 texts.append(obj[\"text\"])\n",
    "#                 count += 1\n",
    "#             text_count.append(count)\n",
    "#             tcount += 1\n",
    "\n",
    "#         # Batch process the texts to classify emotions\n",
    "#         batch_results = classifier(texts, batch_size=16)  # Adjust batch size based on GPU memory\n",
    "\n",
    "#         # print(len(text_count))\n",
    "#         # print(tcount)\n",
    "#     with open(os.path.join(inputdir, file)) as f1, open(os.path.join(outputdir, file), 'w') as f2:\n",
    "        \n",
    "#         # Update the original data with emotion results\n",
    "#         text_index = 0\n",
    "#         # lineCount = 0\n",
    "#         for line, count in zip(f1, text_count):\n",
    "#             text_entry = json.loads(line)\n",
    "#             for obj in text_entry['output']['Objects']:\n",
    "#                 obj[\"emotion\"] = batch_results[text_index]\n",
    "#                 # print(batch_results[text_index])\n",
    "#                 text_index += 1\n",
    "#             # Write the updated data back to the output file\n",
    "#             f2.write(json.dumps(text_entry) + '\\n')\n",
    "#         # Write the updated data back to the output file\n",
    "#     print(f\"File {file} written\")\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File twitter_stream_2020_01_07_filtered.json already processed\n",
      "File twitter-stream-2021-05-24_filtered.json already processed\n",
      "File twitter-stream-20211004_filtered.json already processed\n",
      "File twitter-stream-2020-09-22_filtered.json already processed\n",
      "File twitter-stream-2020-11-21_filtered.json already processed\n",
      "File twitter-stream-2020-07-12_filtered.json already processed\n",
      "File twitter-stream-2020-12-21_filtered.json already processed\n",
      "File twitter-stream-20211028_filtered.json already processed\n",
      "File twitter-stream-2021-02-25_filtered.json already processed\n",
      "File twitter-stream-2020-10-16_filtered.json already processed\n",
      "File twitter-stream-2021-03-03_filtered.json already processed\n",
      "File twitter-stream-2021-07-05_filtered.json already processed\n",
      "File twitter-stream-2020-12-03_filtered.json already processed\n",
      "File twitter-stream-20211022_filtered.json already processed\n",
      "File twitter-stream-20211127_filtered.json already processed\n",
      "File twitter-stream-2021-01-26_filtered.json already processed\n",
      "File twitter_stream_2020_06_12_filtered.json already processed\n",
      "File twitter-stream-20211016_filtered.json already processed\n",
      "File twitter_stream_2020_06_24_filtered.json already processed\n",
      "File twitter-stream-20211103_filtered.json already processed\n",
      "File twitter-stream-2020-11-09_filtered.json already processed\n",
      "File twitter-stream-2020-08-17_filtered.json already processed\n",
      "File twitter-stream-2020-12-15_filtered.json already processed\n",
      "File twitter_stream_2020_05_01_filtered.json already processed\n",
      "File twitter-stream-2021-06-17_filtered.json already processed\n",
      "File twitter-stream-2021-02-19_filtered.json already processed\n",
      "File twitter-stream-2021-08-05_filtered.json already processed\n",
      "File twitter-stream-2021-08-17_filtered.json already processed\n",
      "File twitter-stream-2020-12-09_filtered.json already processed\n",
      "File twitter-stream-2021-03-09_filtered.json already processed\n",
      "File twitter-stream-20210910_filtered.json already processed\n",
      "File twitter-stream-2020-11-03_filtered.json already processed\n",
      "File twitter_stream_2020_05_19_filtered.json already processed\n",
      "File twitter-stream-20210904_filtered.json already processed\n",
      "File twitter-stream-20210922_filtered.json already processed\n",
      "File twitter-stream-2020-10-10_filtered.json already processed\n",
      "File twitter-stream-20211227_filtered.json already processed\n",
      "File twitter-stream-2020-08-05_filtered.json already processed\n",
      "File twitter-stream-2021-05-18_filtered.json already processed\n",
      "File twitter-stream-2021-02-01_filtered.json already processed\n",
      "File twitter-stream-20210823_filtered.json already processed\n",
      "File twitter_stream_2020_04_14_filtered.json already processed\n",
      "File twitter_stream_2020_06_06_filtered.json already processed\n",
      "File twitter-stream-2020-12-27_filtered.json already processed\n",
      "File twitter-stream-2020-07-24_filtered.json already processed\n",
      "File twitter-stream-20211215_filtered.json already processed\n",
      "File twitter-stream-2020-07-30_filtered.json already processed\n",
      "File twitter-stream-2021-02-07_filtered.json already processed\n",
      "File twitter_stream_2020_03_03_filtered.json already processed\n",
      "File twitter_stream_2020_04_26_filtered.json already processed\n",
      "File twitter-stream-20210829_filtered.json already processed\n",
      "File twitter-stream-2020-09-04_filtered.json already processed\n",
      "File twitter-stream-2020-08-29_filtered.json already processed\n",
      "File twitter-stream-20211209_filtered.json already processed\n",
      "File twitter-stream-2020-10-22_filtered.json already processed\n",
      "File twitter_stream_2020_03_21_filtered.json already processed\n",
      "File twitter-stream-20210916_filtered.json already processed\n",
      "File twitter-stream-2021-06-05_filtered.json already processed\n",
      "File twitter-stream-2021-05-30_filtered.json already processed\n",
      "File twitter-stream-2020-09-28_filtered.json already processed\n",
      "File twitter-stream-20211010_filtered.json already processed\n",
      "File twitter-stream-2021-07-24_filtered.json written\n",
      "File twitter-stream-2021-03-21_filtered.json written\n",
      "File twitter-stream-2021-03-27_filtered.json written\n",
      "File twitter-stream-2021-06-23_filtered.json written\n",
      "File twitter-stream-2021-04-02_filtered.json written\n",
      "File twitter-stream-20211203_filtered.json written\n",
      "File twitter-stream-2021-05-12_filtered.json written\n",
      "File twitter-stream-2021-07-18_filtered.json written\n",
      "File twitter-stream-20211115_filtered.json written\n",
      "File twitter-stream-2021-08-11_filtered.json written\n",
      "File twitter-stream-2020-11-27_filtered.json written\n",
      "File twitter-stream-20211121_filtered.json written\n",
      "File twitter-stream-20210928_filtered.json written\n",
      "File twitter-stream-2021-06-29_filtered.json written\n",
      "File twitter-stream-2020-07-06_filtered.json written\n",
      "File twitter-stream-2021-04-30_filtered.json written\n",
      "File twitter-stream-2020-08-23_filtered.json written\n",
      "File twitter-stream-2020-09-16_filtered.json written\n",
      "File twitter-stream-2020-08-11_filtered.json written\n",
      "File twitter-stream-2020-10-28_filtered.json written\n",
      "File twitter-stream-20211221_filtered.json written\n",
      "File twitter-stream-2021-01-02_filtered.json written\n",
      "File twitter_stream_2020_01_01_filtered.json written\n",
      "File twitter-stream-2020-11-15_filtered.json written\n",
      "File twitter-stream-2021-04-08_filtered.json written\n",
      "File twitter-stream-2021-07-30_filtered.json written\n",
      "File twitter_stream_2020_06_30_filtered.json written\n",
      "File twitter-stream-2021-05-06_filtered.json written\n",
      "File twitter-stream-20211109_filtered.json written\n",
      "File twitter-stream-2021-06-11_filtered.json written\n",
      "File twitter_stream_2020_05_22_filtered.json written\n",
      "File twitter-stream-2021-07-12_filtered.json written\n",
      "File twitter-stream-2021-02-13_filtered.json written\n",
      "File twitter-stream-2021-03-15_filtered.json written\n",
      "File twitter-stream-2020-09-10_filtered.json written\n",
      "File twitter_stream_2020_05_13_filtered.json written\n",
      "File twitter-stream-2020-07-18_filtered.json written\n",
      "File twitter_stream_2020_06_18_filtered.json written\n",
      "File twitter_stream_2020_05_07_filtered.json written\n",
      "File twitter-stream-2020-10-04_filtered.json written\n",
      "File twitter_stream_2020_05_31_filtered.json written\n",
      "File twitter_stream_2020_04_20_filtered.json written\n",
      "File twitter_stream_2020_04_08_filtered.json written\n",
      "File twitter_stream_2020_03_27_filtered.json written\n",
      "File twitter_stream_2020_03_15_filtered.json written\n",
      "Processing complete for files: ['twitter_stream_2020_01_07_filtered.json', 'twitter-stream-2021-05-24_filtered.json', 'twitter-stream-20211004_filtered.json', 'twitter-stream-2020-09-22_filtered.json', 'twitter-stream-2020-11-21_filtered.json', 'twitter-stream-2020-07-12_filtered.json', 'twitter-stream-2020-12-21_filtered.json', 'twitter-stream-20211028_filtered.json', 'twitter-stream-2021-02-25_filtered.json', 'twitter-stream-2020-10-16_filtered.json', 'twitter-stream-2021-03-03_filtered.json', 'twitter-stream-2021-07-05_filtered.json', 'twitter-stream-2020-12-03_filtered.json', 'twitter-stream-20211022_filtered.json', 'twitter-stream-20211127_filtered.json', 'twitter-stream-2021-01-26_filtered.json', 'twitter_stream_2020_06_12_filtered.json', 'twitter-stream-20211016_filtered.json', 'twitter_stream_2020_06_24_filtered.json', 'twitter-stream-20211103_filtered.json', 'twitter-stream-2020-11-09_filtered.json', 'twitter-stream-2021-08-17_filtered.json', 'twitter-stream-2020-08-17_filtered.json', 'twitter-stream-2020-12-15_filtered.json', 'twitter_stream_2020_05_01_filtered.json', 'twitter-stream-2021-06-17_filtered.json', 'twitter-stream-2021-02-19_filtered.json', 'twitter-stream-2021-08-05_filtered.json', 'twitter-stream-2020-10-10_filtered.json', 'twitter-stream-2020-12-09_filtered.json', 'twitter-stream-2021-03-09_filtered.json', 'twitter-stream-20210910_filtered.json', 'twitter-stream-2020-11-03_filtered.json', 'twitter_stream_2020_05_19_filtered.json', 'twitter-stream-20210904_filtered.json', 'twitter-stream-20210922_filtered.json', 'twitter-stream-20211227_filtered.json', 'twitter-stream-2020-08-05_filtered.json', 'twitter-stream-2021-05-18_filtered.json', 'twitter-stream-2021-02-01_filtered.json', 'twitter-stream-20210823_filtered.json', 'twitter_stream_2020_04_14_filtered.json', 'twitter_stream_2020_06_06_filtered.json', 'twitter-stream-2020-12-27_filtered.json', 'twitter-stream-2020-07-24_filtered.json', 'twitter-stream-20211215_filtered.json', 'twitter-stream-2020-07-30_filtered.json', 'twitter-stream-2021-02-07_filtered.json', 'twitter_stream_2020_03_03_filtered.json', 'twitter_stream_2020_04_26_filtered.json', 'twitter-stream-20210829_filtered.json', 'twitter-stream-2020-09-04_filtered.json', 'twitter-stream-2020-08-29_filtered.json', 'twitter-stream-20211209_filtered.json', 'twitter-stream-2020-10-22_filtered.json', 'twitter-stream-20211010_filtered.json', 'twitter_stream_2020_03_21_filtered.json', 'twitter-stream-20210916_filtered.json', 'twitter-stream-2021-06-05_filtered.json', 'twitter-stream-2021-05-30_filtered.json', 'twitter-stream-2020-09-28_filtered.json', 'twitter_stream_2020_03_15_filtered.json', 'twitter-stream-2021-06-23_filtered.json', 'twitter_stream_2020_06_30_filtered.json', 'twitter-stream-2021-07-18_filtered.json', 'twitter_stream_2020_05_22_filtered.json', 'twitter-stream-2020-11-27_filtered.json', 'twitter-stream-2021-08-11_filtered.json', 'twitter-stream-2021-03-27_filtered.json', 'twitter-stream-2020-07-06_filtered.json', 'twitter-stream-2021-07-24_filtered.json', 'twitter-stream-2020-09-16_filtered.json', 'twitter-stream-2021-03-21_filtered.json', 'twitter-stream-2020-08-11_filtered.json', 'twitter_stream_2020_05_13_filtered.json', 'twitter-stream-20211203_filtered.json', 'twitter-stream-2020-10-28_filtered.json', 'twitter-stream-2020-10-04_filtered.json', 'twitter-stream-2020-08-23_filtered.json', 'twitter-stream-2021-05-12_filtered.json', 'twitter-stream-2021-02-13_filtered.json', 'twitter-stream-2021-04-02_filtered.json', 'twitter-stream-20211115_filtered.json', 'twitter-stream-20210928_filtered.json', 'twitter_stream_2020_03_27_filtered.json', 'twitter-stream-20211121_filtered.json', 'twitter-stream-2020-09-10_filtered.json', 'twitter_stream_2020_05_07_filtered.json', 'twitter-stream-2021-04-30_filtered.json', 'twitter-stream-2020-11-15_filtered.json', 'twitter-stream-2021-06-29_filtered.json', 'twitter_stream_2020_04_08_filtered.json', 'twitter-stream-2021-01-02_filtered.json', 'twitter-stream-20211221_filtered.json', 'twitter-stream-2021-07-30_filtered.json', 'twitter-stream-2021-04-08_filtered.json', 'twitter-stream-2021-05-06_filtered.json', 'twitter-stream-2020-07-18_filtered.json', 'twitter_stream_2020_04_20_filtered.json', 'twitter_stream_2020_05_31_filtered.json', 'twitter-stream-2021-06-11_filtered.json', 'twitter-stream-2021-07-12_filtered.json', 'twitter_stream_2020_01_01_filtered.json', 'twitter-stream-20211109_filtered.json', 'twitter_stream_2020_06_18_filtered.json', 'twitter-stream-2021-03-15_filtered.json']\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "\n",
    "def process_file(file):\n",
    "    \"\"\"Process a single file to classify emotions.\"\"\"\n",
    "    input_path = os.path.join(inputdir, file)\n",
    "    output_path = os.path.join(outputdir, file)\n",
    "\n",
    "    # Skip files that are already processed\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"File {file} already processed\")\n",
    "        return file\n",
    "\n",
    "    # Read the input file and collect texts for classification\n",
    "    texts = []\n",
    "    text_count = []\n",
    "    with open(input_path, 'r') as f:\n",
    "        for line in f:\n",
    "            text_entry = json.loads(line)\n",
    "            count = 0\n",
    "            for obj in text_entry['output']['Objects']:\n",
    "                texts.append(obj[\"text\"])\n",
    "                count += 1\n",
    "            text_count.append(count)\n",
    "\n",
    "    # Classify emotions in batch\n",
    "    batch_results = classifier(texts, batch_size=32)  # Adjust batch size based on GPU memory\n",
    "    print(\"Done classifying emotions for file\", file)\n",
    "\n",
    "    # Update the original data with classification results\n",
    "    text_index = 0\n",
    "    with open(input_path, 'r') as f, open(output_path, 'w') as output_file:\n",
    "        for line, count in zip(f, text_count):\n",
    "            text_entry = json.loads(line)\n",
    "            for obj in text_entry['output']['Objects']:\n",
    "                obj[\"emotion\"] = batch_results[text_index]\n",
    "                text_index += 1\n",
    "            output_file.write(json.dumps(text_entry) + '\\n')\n",
    "    print(f\"File {file} written\")\n",
    "    return file\n",
    "\n",
    "def main():\n",
    "    # Get the list of files to process\n",
    "    files = os.listdir(inputdir)\n",
    "\n",
    "    # Use ThreadPoolExecutor to process files in parallel\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_file, files))\n",
    "\n",
    "    print(\"Processing complete for files:\", results)\n",
    "\n",
    "# Run the main function\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File twitter_stream_2020_01_07_filtered.json already processed\n",
      "File twitter-stream-2021-05-24_filtered.json already processed\n",
      "File twitter-stream-20211004_filtered.json already processed\n",
      "File twitter-stream-2020-09-22_filtered.json already processed\n",
      "File twitter-stream-2020-11-21_filtered.json already processed\n",
      "File twitter-stream-2020-07-12_filtered.json already processed\n",
      "File twitter-stream-2020-12-21_filtered.json already processed\n",
      "File twitter-stream-20211028_filtered.json already processed\n",
      "File twitter-stream-2021-02-25_filtered.json already processed\n",
      "File twitter-stream-2020-10-16_filtered.json already processed\n",
      "File twitter-stream-2021-03-03_filtered.json already processed\n",
      "File twitter-stream-2021-07-05_filtered.json already processed\n",
      "File twitter-stream-2020-12-03_filtered.json already processed\n",
      "File twitter-stream-20211022_filtered.json already processed\n",
      "File twitter-stream-20211127_filtered.json already processed\n",
      "File twitter-stream-2021-01-26_filtered.json already processed\n",
      "File twitter_stream_2020_06_12_filtered.json already processed\n",
      "File twitter-stream-20211016_filtered.json already processed\n",
      "File twitter_stream_2020_06_24_filtered.json already processed\n",
      "File twitter-stream-20211103_filtered.json already processed\n",
      "File twitter-stream-2020-11-09_filtered.json already processed\n",
      "File twitter-stream-2021-08-17_filtered.json already processed\n",
      "File twitter-stream-2020-08-17_filtered.json already processed\n",
      "File twitter-stream-2020-12-15_filtered.json already processed\n",
      "File twitter_stream_2020_05_01_filtered.json already processed\n",
      "File twitter-stream-2021-06-17_filtered.json already processed\n",
      "File twitter-stream-2021-02-19_filtered.json already processed\n",
      "File twitter-stream-2021-08-05_filtered.json already processed\n",
      "File twitter-stream-2020-10-10_filtered.json already processed\n",
      "File twitter-stream-2020-12-09_filtered.json already processed\n",
      "File twitter-stream-2021-03-09_filtered.json already processed\n",
      "File twitter-stream-20210910_filtered.json already processed\n",
      "File twitter-stream-2020-11-03_filtered.json already processed\n",
      "File twitter_stream_2020_05_19_filtered.json already processed\n",
      "File twitter-stream-20210904_filtered.json already processed\n",
      "File twitter-stream-20210922_filtered.json already processed\n",
      "File twitter-stream-20211227_filtered.json already processed\n",
      "File twitter-stream-2020-08-05_filtered.json already processed\n",
      "File twitter-stream-2021-05-18_filtered.json already processed\n",
      "File twitter-stream-2021-02-01_filtered.json already processed\n",
      "File twitter-stream-20210823_filtered.json already processed\n",
      "File twitter_stream_2020_04_14_filtered.json already processed\n",
      "File twitter_stream_2020_06_06_filtered.json already processed\n",
      "File twitter-stream-2020-12-27_filtered.json already processed\n",
      "File twitter-stream-2020-07-24_filtered.json already processed\n",
      "File twitter-stream-20211215_filtered.json already processed\n",
      "File twitter-stream-2020-07-30_filtered.json already processed\n",
      "File twitter-stream-2021-02-07_filtered.json already processed\n",
      "File twitter_stream_2020_03_03_filtered.json already processed\n",
      "File twitter_stream_2020_04_26_filtered.json already processed\n",
      "File twitter-stream-20210829_filtered.json already processed\n",
      "File twitter-stream-2020-09-04_filtered.json already processed\n",
      "File twitter-stream-2020-08-29_filtered.json already processed\n",
      "File twitter-stream-20211209_filtered.json already processed\n",
      "File twitter-stream-2020-10-22_filtered.json already processed\n",
      "File twitter-stream-20211010_filtered.json already processed\n",
      "File twitter_stream_2020_03_21_filtered.json already processed\n",
      "File twitter-stream-20210916_filtered.json already processed\n",
      "File twitter-stream-2021-06-05_filtered.json already processed\n",
      "File twitter-stream-2021-05-30_filtered.json already processed\n",
      "File twitter-stream-2020-09-28_filtered.json already processed\n",
      "File twitter_stream_2020_03_15_filtered.json already processed\n",
      "File twitter-stream-2021-06-23_filtered.json written\n",
      "File twitter_stream_2020_06_30_filtered.json written\n",
      "File twitter-stream-2021-07-18_filtered.json written\n",
      "File twitter_stream_2020_05_22_filtered.json written\n",
      "File twitter-stream-2020-11-27_filtered.json written\n",
      "File twitter-stream-2021-08-11_filtered.json written\n",
      "File twitter-stream-2021-03-27_filtered.json written\n",
      "File twitter-stream-2020-07-06_filtered.json written\n",
      "File twitter-stream-2021-07-24_filtered.json written\n",
      "File twitter-stream-2020-09-16_filtered.json written\n",
      "File twitter-stream-2021-03-21_filtered.json written\n",
      "File twitter-stream-2020-08-11_filtered.json written\n",
      "File twitter_stream_2020_05_13_filtered.json written\n",
      "File twitter-stream-20211203_filtered.json written\n",
      "File twitter-stream-2020-10-28_filtered.json written\n",
      "File twitter-stream-2020-10-04_filtered.json written\n",
      "File twitter-stream-2020-08-23_filtered.json written\n",
      "File twitter-stream-2021-05-12_filtered.json written\n",
      "File twitter-stream-2021-02-13_filtered.json written\n",
      "File twitter-stream-2021-04-02_filtered.json written\n",
      "File twitter-stream-20211115_filtered.json written\n",
      "File twitter-stream-20210928_filtered.json written\n",
      "File twitter_stream_2020_03_27_filtered.json written\n",
      "File twitter-stream-20211121_filtered.json written\n",
      "File twitter-stream-2020-09-10_filtered.json written\n",
      "File twitter_stream_2020_05_07_filtered.json written\n",
      "File twitter-stream-2021-04-30_filtered.json written\n",
      "File twitter-stream-2020-11-15_filtered.json written\n",
      "File twitter-stream-2021-06-29_filtered.json written\n",
      "File twitter_stream_2020_04_08_filtered.json written\n",
      "File twitter-stream-2021-01-02_filtered.json written\n",
      "File twitter-stream-20211221_filtered.json written\n",
      "File twitter-stream-2021-07-30_filtered.json written\n",
      "File twitter-stream-2021-04-08_filtered.json written\n",
      "File twitter-stream-2021-05-06_filtered.json written\n",
      "File twitter-stream-2020-07-18_filtered.json written\n",
      "File twitter_stream_2020_04_20_filtered.json written\n",
      "File twitter_stream_2020_05_31_filtered.json written\n",
      "File twitter-stream-2021-06-11_filtered.json written\n",
      "File twitter-stream-2021-07-12_filtered.json written\n",
      "File twitter_stream_2020_01_01_filtered.json written\n",
      "File twitter-stream-20211109_filtered.json written\n",
      "File twitter_stream_2020_06_18_filtered.json written\n",
      "File twitter-stream-2021-03-15_filtered.json written\n"
     ]
    }
   ],
   "source": [
    "# now doing the emotions processing\n",
    "input_dir = 'data_with_emotions'\n",
    "output_dir = 'data_with_parent_emotions'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "def process_emotions(obj, emots):\n",
    "    if emots[0]['score'] > 0.75:\n",
    "        obj['emotion'].append(change_to_parent_emotion(emots[0]))\n",
    "    else:\n",
    "        for res in emots:\n",
    "            if res['label'] == 'neutral' and res['score'] > 0.5:\n",
    "                obj['emotion'].append(change_to_parent_emotion(res))\n",
    "            elif (res['label'] != 'neutral' and res['score'] > 0.15):\n",
    "                obj['emotion'].append(change_to_parent_emotion(res))\n",
    "\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    if file in os.listdir(output_dir):\n",
    "        print(f\"File {file} already processed\")\n",
    "        continue\n",
    "    with open(os.path.join(input_dir, file)) as f1:\n",
    "        with open(os.path.join(output_dir, file), 'w') as f2:\n",
    "            for line in f1:\n",
    "                text1 = json.loads(line)\n",
    "                for obj in text1['output']['Objects']:\n",
    "                    # keep only text and emotion in the object\n",
    "                    for key in list(obj.keys()):\n",
    "                        if key not in ['text', 'emotion']:\n",
    "                            del obj[key]\n",
    "                    emots = obj['emotion'].copy()\n",
    "                    obj['emotion'] = []\n",
    "                    process_emotions(obj, emots)\n",
    "                json.dump(text1, f2)\n",
    "                f2.write('\\n')\n",
    "    print(f\"File {file} written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now making graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "keywords = [\n",
    "    \"covid-19\", \n",
    "    \"sars\", \n",
    "    \"coronavirus\",\n",
    "    \"corona\" \n",
    "    \"covid19\", \n",
    "    \"SARS-CoV-2\", \n",
    "    \"loss of taste\", \n",
    "    \"quarantine\", \n",
    "    \"isolation\", \n",
    "    \"Pfizer\", \"Moderna\", \"AstraZeneca\", \n",
    "]\n",
    "\n",
    "\n",
    "# Category to emotions mapping\n",
    "category_to_emotions = {\n",
    "    \"Positive or Joyful Emotions\": [\n",
    "        \"admiration\", \"amusement\", \"joy\", \"gratitude\", \"excitement\", \n",
    "        \"optimism\", \"pride\", \"relief\"\n",
    "    ],\n",
    "    \"Negative or Fearful Emotions\": [\n",
    "        \"anger\", \"annoyance\", \"fear\", \"nervousness\", \"sadness\", \n",
    "        \"grief\", \"remorse\"\n",
    "    ],\n",
    "    \"Neutral or Objective Emotions\": [\n",
    "        \"neutral\", \"approval\", \"realization\"\n",
    "    ],  \n",
    "    \"Love and Caring Emotions\": [\n",
    "        \"love\", \"caring\", \"desire\"\n",
    "    ],\n",
    "    \"Surprise or Unexpected Responses\": [\n",
    "        \"surprise\", \"curiosity\", \"confusion\"\n",
    "    ],\n",
    "    \"Displeasure or Disapproval Emotions\": [\n",
    "        \"disappointment\", \"disapproval\", \"disgust\",\"embarrassment\"\n",
    "    ],\n",
    "}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
